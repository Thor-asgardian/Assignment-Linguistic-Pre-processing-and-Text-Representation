{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Linguistic Pre-processing and Text Representation\n",
    "\n",
    "## Instructions\n",
    "- Answer all questions with detailed explanations\n",
    "- Include code examples where applicable\n",
    "- Provide reasoning for your design choices\n",
    "- Each question requires a comprehensive answer demonstrating understanding of concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Multi-level Linguistic Analysis\n",
    "\n",
    "Consider the sentence: \"The company's CEO didn't respond to our meeting invitation.\"\n",
    "\n",
    "Analyze this sentence from four different linguistic perspectives:\n",
    "- **Syntax**: Identify the grammatical structure and phrase composition\n",
    "- **Semantics**: Explain the meaning and relationships between words\n",
    "- **Morphology**: Break down word formations and their components\n",
    "- **Pragmatics**: Discuss the contextual interpretation and implied meaning\n",
    "\n",
    "**Hint**: Consider how each level provides different insights. For morphology, examine words like \"didn't\" and \"invitation\". For pragmatics, think about what this might imply in a business context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Syntax: SVO NP=The company's CEO, VP=didn’t respond, PP=to our meeting invitation\n",
    "Semantics: agent failed to perform responding action\n",
    "Morphology: didn’t=did+not, company's=company+'s, invitation=invite+-tion\n",
    "Pragmatics: implies soft refusal / low priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Pre-processing Pipeline Design\n",
    "\n",
    "You are building a sentiment analysis system for customer reviews from an e-commerce platform. The reviews contain:\n",
    "- Informal language and slang (\"gonna\", \"wanna\", \"u\")\n",
    "- Emojis and special characters\n",
    "- Product codes and prices\n",
    "- Misspellings and typos\n",
    "\n",
    "Design a comprehensive text pre-processing pipeline. For each step (tokenization, normalization, stop-word removal, stemming/lemmatization), explain:\n",
    "1. Why you would include or exclude it\n",
    "2. What specific considerations apply to this use case\n",
    "3. The order of operations and why it matters\n",
    "\n",
    "**Hint**: Consider whether stemming or lemmatization is more appropriate for sentiment analysis. Think about whether removing all special characters is beneficial when emojis carry sentiment information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline:\n",
    "- tokenize\n",
    "- normalize slang → mapping dict\n",
    "- keep emojis (carry sentiment)\n",
    "- lemmatize > stem\n",
    "order: normalize→tokenize→emoji preserve→lemma→vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Stemming vs Lemmatization Trade-offs\n",
    "\n",
    "Consider these sentences:\n",
    "1. \"The meeting was well organized and the organizers did a great job.\"\n",
    "2. \"She is better at organizing than her predecessor was.\"\n",
    "\n",
    "Apply both stemming (Porter Stemmer) and lemmatization to these sentences. Then:\n",
    "- Compare the outputs and explain the differences\n",
    "- Discuss scenarios where stemming would be preferred over lemmatization and vice versa\n",
    "- Analyze the impact on: search engines, text classification, and information retrieval systems\n",
    "\n",
    "**Hint**: Consider computational cost, accuracy, and preservation of meaning. Words like \"better\", \"organizing\", and \"was\" behave differently under stemming vs lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemming crude vs lemma correct base.\n",
    "Use lemma for classification.\n",
    "Example code:\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: POS Tagging for Ambiguity Resolution\n",
    "\n",
    "Examine these ambiguous sentences:\n",
    "1. \"The duck is ready to eat.\"\n",
    "2. \"They can fish.\"\n",
    "3. \"Time flies like an arrow.\"\n",
    "\n",
    "Explain:\n",
    "- How POS tagging helps resolve these ambiguities\n",
    "- The difference between rule-based and probabilistic POS tagging approaches\n",
    "- Which approach would perform better for each sentence and why\n",
    "- Limitations of both approaches\n",
    "\n",
    "**Hint**: Consider how context and word order influence tagging. Think about the Hidden Markov Model approach for probabilistic tagging vs pattern-matching rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS tagging resolves role: bank NN vs VB.\n",
    "spaCy example:\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Named Entity Recognition System Design\n",
    "\n",
    "You need to build an NER system for extracting information from medical reports. The text contains:\n",
    "- Disease names (\"Type 2 Diabetes\", \"COVID-19\")\n",
    "- Medication names (\"Metformin\", \"Ibuprofen 200mg\")\n",
    "- Dosages and measurements\n",
    "- Doctor and patient names\n",
    "- Hospital names and dates\n",
    "\n",
    "Compare dictionary-based and CRF-based NER methods for this application:\n",
    "- Advantages and disadvantages of each approach\n",
    "- How would you handle new drug names not in the dictionary?\n",
    "- What features would you use in a CRF model?\n",
    "- How would you combine both approaches for optimal results?\n",
    "\n",
    "**Hint**: Consider that medical terminology is specialized but relatively standardized. Think about feature engineering for CRF models (capitalization, word shape, surrounding words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary=precise; CRF=generalise OOV. Combine CRF→dict validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: N-gram Language Models and Perplexity\n",
    "\n",
    "Given a small corpus:\n",
    "```\n",
    "\"I love machine learning\"\n",
    "\"I love deep learning\"\n",
    "\"Machine learning is fascinating\"\n",
    "\"Deep learning is powerful\"\n",
    "```\n",
    "\n",
    "a) Build a bigram language model and calculate probabilities for:\n",
    "   - \"I love natural learning\"\n",
    "   - \"Machine learning is powerful\"\n",
    "\n",
    "b) Explain the zero-probability problem and demonstrate:\n",
    "   - How Laplace smoothing addresses it\n",
    "   - The concept of backoff strategies\n",
    "   - How to calculate and interpret perplexity\n",
    "\n",
    "c) Discuss why lower perplexity indicates a better language model.\n",
    "\n",
    "**Hint**: For unseen bigrams like \"natural learning\", consider what probability would be assigned without smoothing. Calculate perplexity as a measure of how \"surprised\" the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Higher n reduces perplexity until sparse.\n",
    "Laplace smoothing fixes zero probability.\n",
    "PP lower = better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Bag-of-Words vs TF-IDF Analysis\n",
    "\n",
    "Consider three documents:\n",
    "- Doc1: \"Machine learning is a subset of artificial intelligence\"\n",
    "- Doc2: \"Deep learning is a subset of machine learning\"\n",
    "- Doc3: \"Artificial intelligence and machine learning are transforming industries\"\n",
    "\n",
    "a) Construct the BoW representation and TF-IDF vectors for all documents\n",
    "\n",
    "b) Calculate cosine similarity between documents using both representations\n",
    "\n",
    "c) Explain:\n",
    "   - Why the similarity scores differ between BoW and TF-IDF\n",
    "   - Which representation better captures document similarity for:\n",
    "     - Information retrieval\n",
    "     - Document clustering\n",
    "     - Topic modeling\n",
    "   - Limitations of both approaches\n",
    "\n",
    "**Hint**: Consider how TF-IDF downweights common terms like \"is\" and \"a\". Think about what information is lost (word order, context, semantics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW=counts; TFIDF=downweight common → better similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Word2Vec Architectures Deep Dive\n",
    "\n",
    "Explain the Word2Vec model by addressing:\n",
    "\n",
    "a) **CBOW (Continuous Bag of Words)**:\n",
    "   - Architecture and training objective\n",
    "   - How context words predict the target word\n",
    "   - Best use cases\n",
    "\n",
    "b) **Skip-gram**:\n",
    "   - Architecture and training objective\n",
    "   - How target word predicts context words\n",
    "   - Best use cases\n",
    "\n",
    "c) For the sentence \"The quick brown fox jumps over the lazy dog\" (window size = 2):\n",
    "   - Show training examples for both CBOW and Skip-gram when target word is \"fox\"\n",
    "   - Explain which architecture works better for:\n",
    "     - Small datasets\n",
    "     - Rare words\n",
    "     - Frequent words\n",
    "\n",
    "**Hint**: CBOW is faster and works well with frequent words, while Skip-gram is better for rare words and smaller datasets. Consider the number of training instances generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW fast on big data; Skipgram better rare.\n",
    "small=skipgram large=cbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: GloVe vs FastText Comparison\n",
    "\n",
    "Compare and contrast GloVe and FastText embedding techniques:\n",
    "\n",
    "a) **Training methodology**:\n",
    "   - How does GloVe use global co-occurrence statistics?\n",
    "   - How does FastText incorporate subword information?\n",
    "\n",
    "b) **Handling Out-of-Vocabulary (OOV) words**:\n",
    "   - Given the trained words: \"playing\", \"player\", \"played\"\n",
    "   - How would each model handle the unseen word \"gameplay\"?\n",
    "   - Which model is more suitable for morphologically rich languages (e.g., German, Turkish)?\n",
    "\n",
    "c) **Practical considerations**:\n",
    "   - Training time and computational requirements\n",
    "   - Model size and memory footprint\n",
    "   - Performance on rare and misspelled words\n",
    "\n",
    "**Hint**: FastText breaks words into character n-grams (e.g., \"playing\" → \"<pl\", \"pla\", \"lay\", \"ayi\", \"yin\", \"ing\", \"ng>\"). GloVe uses matrix factorization on co-occurrence counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe global stats; FastText subwords → handles OOV, better morph languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10: Classical vs Distributed Representations - Application Perspective\n",
    "\n",
    "You are tasked with building three different NLP applications:\n",
    "\n",
    "1. **Legal document search engine** (searching through contracts and legal texts)\n",
    "2. **Chatbot intent classification** (understanding user queries)\n",
    "3. **Academic paper recommendation system** (suggesting related research papers)\n",
    "\n",
    "For each application:\n",
    "\n",
    "a) Decide whether to use classical representations (BoW/TF-IDF) or distributed representations (Word2Vec/GloVe/FastText)\n",
    "\n",
    "b) Justify your choice by considering:\n",
    "   - Semantic similarity requirements\n",
    "   - Vocabulary size and domain specificity\n",
    "   - Training data availability\n",
    "   - Computational constraints\n",
    "   - Interpretability needs\n",
    "\n",
    "c) Discuss hybrid approaches: Could combining both representation types improve performance? How?\n",
    "\n",
    "**Hint**: Legal documents might require exact term matching, while chatbots benefit from semantic understanding. Consider that classical methods are sparse and interpretable, while distributed representations are dense and capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Legal=TFIDF exact; chatbot=embeddings; recommender=hybrid concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guidelines\n",
    "\n",
    "- Complete all questions in this notebook\n",
    "- Include code implementations where applicable (using NLTK, spaCy, scikit-learn, or gensim)\n",
    "- Provide clear explanations and reasoning\n",
    "- Add visualizations if they help explain your answers\n",
    "- Ensure your code is properly commented"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
